{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb504681-f614-4f6f-81f2-dc4bc1b50920",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb443b5c-017f-41ed-a149-23b413c17d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49bb890-69a8-41c6-a659-c87450af1bf4",
   "metadata": {},
   "source": [
    "# Step 2: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e10032ec-57a8-400c-b060-a4a5b4699743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\user\\Desktop\\spam.csv\", encoding='latin-1') \n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058bf2d-9648-491f-8b1a-16aa99ae4eb4",
   "metadata": {},
   "source": [
    "# Step 3: Drop Unnecessary Columns and Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3683ac3-ccbc-4129-9f63-80b3885c87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns and rename columns\n",
    "df = df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])\n",
    "df = df.rename(columns={'v1': 'label', 'v2': 'message'})\n",
    "\n",
    "# Display the first few rows to confirm changes\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f09f38b-5deb-41c8-84b3-8b665cdb31cc",
   "metadata": {},
   "source": [
    "# Step 4: Convert Labels to Binary Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7437e6f0-c168-432d-a94d-7c2757263fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to binary values: 'spam' -> 1, 'ham' -> 0\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c918cf-fdea-4054-98e7-d929b4933272",
   "metadata": {},
   "source": [
    "# Step 5: Preprocess the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa1af422-e331-4af6-a196-fb1137bb1b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            message\n",
      "0      0  go jurong point crazi avail bugi n great world...\n",
      "1      0                              ok lar joke wif u oni\n",
      "2      1  free entri wkli comp win fa cup final tkt st m...\n",
      "3      0                u dun say earli hor u c alreadi say\n",
      "4      0          nah dont think goe usf live around though\n"
     ]
    }
   ],
   "source": [
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Enhanced preprocess text function\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    # Tokenize text\n",
    "    words = text.split()\n",
    "    # Remove stopwords and apply stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['message'] = df['message'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows to confirm preprocessing\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec33093-4dc7-4253-8cb5-da2a2b8531f5",
   "metadata": {},
   "source": [
    "# Step 6: Add Additional Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e733a53d-eddd-40f1-a6b6-b0281f8b5861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            message  message_length\n",
      "0      0  go jurong point crazi avail bugi n great world...              76\n",
      "1      0                              ok lar joke wif u oni              21\n",
      "2      1  free entri wkli comp win fa cup final tkt st m...             103\n",
      "3      0                u dun say earli hor u c alreadi say              35\n",
      "4      0          nah dont think goe usf live around though              41\n"
     ]
    }
   ],
   "source": [
    "# Add a new feature for message length\n",
    "df['message_length'] = df['message'].apply(len)\n",
    "\n",
    "# Display the first few rows to confirm the new feature\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c33202-f251-4d90-9179-150e48171479",
   "metadata": {},
   "source": [
    "# Step 7: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d175c4f-8d4b-4fbc-85b8-a46cd544af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_text = vectorizer.fit_transform(df['message'])\n",
    "\n",
    "# Convert to DataFrame and ensure all column names are strings\n",
    "X_text_df = pd.DataFrame(X_text.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "X_text_df.columns = X_text_df.columns.astype(str)\n",
    "\n",
    "# Combine text features with message length\n",
    "X = pd.concat([X_text_df, df['message_length'].reset_index(drop=True)], axis=1)\n",
    "y = df['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58014553-f789-4029-80e2-8a33d2ce3a64",
   "metadata": {},
   "source": [
    "# Step 8: Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c200ffe-4d2d-4791-b31e-8befa7a4c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da4ca5-49ed-40c0-a3c8-7c85cb9f70e1",
   "metadata": {},
   "source": [
    "# Step 9: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "122d7840-ab52-48c6-8717-a7484f98030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0]  # Add more values if needed\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ba29af-6e5f-4208-af9c-288a5dad041c",
   "metadata": {},
   "source": [
    "# Step 10: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89d146a9-81c9-4949-bcb9-6f43cd275859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.979372197309417\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       965\n",
      "           1       0.94      0.91      0.92       150\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.96      0.95      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a22566-107e-4a27-9f4c-0bf1ba093b30",
   "metadata": {},
   "source": [
    "# Step 11: Predict on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb11bfda-e0fb-43b6-9bff-caab3af7a3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New message prediction: spam\n",
      "New message prediction: ham\n"
     ]
    }
   ],
   "source": [
    "# Function to predict if a new message is spam or ham\n",
    "def predict_spam(text):\n",
    "    text = preprocess_text(text)\n",
    "    text_tfidf = vectorizer.transform([text])\n",
    "    text_tfidf_df = pd.DataFrame(text_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    text_tfidf_df.columns = text_tfidf_df.columns.astype(str)\n",
    "    message_length = len(text)\n",
    "    text_features = pd.concat([text_tfidf_df, pd.DataFrame([message_length], columns=['message_length'])], axis=1)\n",
    "    prediction = best_model.predict(text_features)\n",
    "    return 'spam' if prediction == 1 else 'ham'\n",
    "\n",
    "# Example prediction           \n",
    "new_message = \"Congratulations! You've won a free ticket to the Bahamas. Call now!\"\n",
    "new_message_2 = \"I have to inform you that tomorrow is holiday\"\n",
    "print(f'New message prediction: {predict_spam(new_message)}')\n",
    "print(f'New message prediction: {predict_spam(new_message_2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01f4c39a-a17c-437b-be5b-7b5156513b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "718447ff-44d0-4687-b246-3e661e118ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4293b79b-5ea4-4667-b85f-412148d779be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
